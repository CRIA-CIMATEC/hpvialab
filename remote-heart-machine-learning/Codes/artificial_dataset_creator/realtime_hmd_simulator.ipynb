{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Code from https://github.com/sv07i/Face-Detection/blob/main/Face%20Detection.py based on:\n",
        "# https://www.computervision.zone/courses/468-face-landmarks/ and \n",
        "# https://www.youtube.com/watch?v=V9bzew8A1tc\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import time\n",
        "import numpy as np\n",
        "from mesh_points import MESH_ANNOTATIONS\n",
        "\n",
        "pTime = 0\n",
        "# cap = cv2.VideoCapture('/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/MR-NIRP_Indoor/Subject1_still/IR_20s/Frame%05d.pgm')\n",
        "cap = cv2.VideoCapture('/home/desafio01/Videos/Webcam/2021-05-14-190903.webm')\n",
        "# cap = cv2.VideoCapture(0)\n",
        "\n",
        "mpDraw = mp.solutions.drawing_utils\n",
        "mpFaceMesh = mp.solutions.face_mesh\n",
        "faceMesh = mpFaceMesh.FaceMesh(max_num_faces=1)\n",
        "drawspec = mpDraw.DrawingSpec(thickness=1, circle_radius=1)\n",
        "blank = None\n",
        "landmarks_ids = []\n",
        "# landmarks_ids += MESH_ANNOTATIONS['bottom_face']\n",
        "# landmarks_ids += MESH_ANNOTATIONS['leftEye']\n",
        "# landmarks_ids += MESH_ANNOTATIONS['rightEye']\n",
        "# landmarks_ids += MESH_ANNOTATIONS['mouth']\n",
        "# landmarks_ids += MESH_ANNOTATIONS['nose']\n",
        "landmarks_ids_right = MESH_ANNOTATIONS['rightEye']\n",
        "landmarks_ids_left = MESH_ANNOTATIONS['leftEye']\n",
        "scale = 1\n",
        "filter_landmarks = False\n",
        "lock_face = True\n",
        "\n",
        "while True:\n",
        "    ret, img = cap.read()\n",
        "\n",
        "    if ret == False:\n",
        "        break\n",
        "\n",
        "    blank = np.ones(tuple([i * scale for i in img.shape[:2]])) * 255\n",
        "\n",
        "    desiredLeftEye=(0.35, 0.35)\n",
        "    desiredFaceWidth=img.shape[1]\n",
        "    desiredFaceHeight=img.shape[0]\n",
        "    if desiredFaceHeight is None:\n",
        "        desiredFaceHeight = desiredFaceWidth\n",
        "\n",
        "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = faceMesh.process(imgRGB)\n",
        "    if results.multi_face_landmarks:\n",
        "        for faceLms in results.multi_face_landmarks:\n",
        "            if lock_face == False:\n",
        "                landmarks = []\n",
        "                if filter_landmarks:\n",
        "                    for j, i in enumerate(landmarks_ids):\n",
        "                        lm = faceLms.landmark[i]\n",
        "                        h, w, _ = img.shape\n",
        "                        x, y = int(lm.x * w) * scale, int(lm.y * h) * scale\n",
        "                        cv2.circle(blank, (x, y), 1, (0, 0, 255), -1)\n",
        "                        landmarks.append((x , y ))\n",
        "                        # cv2.putText(blank, str(i), (x, y), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
        "                else:\n",
        "                    for j, lm in enumerate(faceLms.landmark):\n",
        "                        h, w, _ = img.shape\n",
        "                        x, y = int(lm.x * w) * scale, int(lm.y * h) * scale\n",
        "                        landmarks.append((x , y ))\n",
        "                        cv2.circle(blank, (x, y), 1, (0, 0, 255), -1)\n",
        "                        # cv2.putText(blank, str(j), (x, y), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
        "            else:\n",
        "                landmarks_right = []\n",
        "                for j, i in enumerate(landmarks_ids_right):\n",
        "                    lm = faceLms.landmark[i]\n",
        "                    h, w, _ = img.shape\n",
        "                    x, y = int(lm.x * w), int(lm.y * h)\n",
        "                    cv2.circle(blank, (x, y), 1, (0, 0, 255), -1)\n",
        "                    landmarks_right.append((x, y))\n",
        "                \n",
        "                landmarks_left = []\n",
        "                for j, i in enumerate(landmarks_ids_left):\n",
        "                    lm = faceLms.landmark[i]\n",
        "                    h, w, _ = img.shape\n",
        "                    x, y = int(lm.x * w), int(lm.y * h)\n",
        "                    cv2.circle(blank, (x, y), 1, (0, 0, 255), -1)\n",
        "                    landmarks_left.append((x, y))\n",
        "\n",
        "                # compute the center of mass for each eye\n",
        "                leftEyeCenter = np.array(landmarks_left).mean(axis=0).astype(\"int\")\n",
        "                rightEyeCenter = np.array(landmarks_right).mean(axis=0).astype(\"int\")\n",
        "                # compute the angle between the eye centroids\n",
        "                dY = rightEyeCenter[1] - leftEyeCenter[1]\n",
        "                dX = rightEyeCenter[0] - leftEyeCenter[0]\n",
        "                angle = np.degrees(np.arctan2(dY, dX)) - 180\n",
        "                angle = 0\n",
        "\n",
        "                # compute the desired right eye x-coordinate based on the\n",
        "                # desired x-coordinate of the left eye\n",
        "                desiredRightEyeX = 1.0 - desiredLeftEye[0]\n",
        "                # determine the scale of the new resulting image by taking\n",
        "                # the ratio of the distance between eyes in the *current*\n",
        "                # image to the ratio of distance between eyes in the\n",
        "                # *desired* image\n",
        "                dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
        "                desiredDist = (desiredRightEyeX - desiredLeftEye[0])\n",
        "                desiredDist *= desiredFaceWidth\n",
        "                scale = desiredDist / dist\n",
        "                scale = 1\n",
        "\n",
        "                # compute center (x, y)-coordinates (i.e., the median point)\n",
        "                # between the two eyes in the input image\n",
        "                eyesCenter = ((leftEyeCenter[0] + rightEyeCenter[0]) // 2,\n",
        "                    (leftEyeCenter[1] + rightEyeCenter[1]) // 2)\n",
        "                # grab the rotation matrix for rotating and scaling the face\n",
        "                M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
        "                # update the translation component of the matrix\n",
        "                tX = desiredFaceWidth * 0.5\n",
        "                tY = desiredFaceHeight * 0.2 # desiredLeftEye[1]\n",
        "                M[0, 2] += (tX - eyesCenter[0])\n",
        "                M[1, 2] += (tY - eyesCenter[1])\n",
        "\n",
        "                # apply the affine transformation\n",
        "                (w, h) = (desiredFaceWidth, desiredFaceHeight)\n",
        "                img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC)\n",
        "                cv2.putText(blank, f'Tx: {tX} Ty: {tY}', (40, 150), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0), 2)\n",
        "                cv2.putText(blank, f'eyesCenter[0]: {eyesCenter[0]} eyesCenter[1]: {eyesCenter[1]}', (40, 250), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0), 2)\n",
        "                cv2.putText(blank, f'desiredFaceWidth: {desiredFaceWidth} desiredFaceHeight: {desiredFaceHeight}', (40, 350), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0), 2)\n",
        "\n",
        "                # img = img[int(tY - eyesCenter[1]):, :] if int(tY - eyesCenter[1]) > 0 else img[:int(tY - eyesCenter[1]), :]\n",
        "                # img = img[:, int(tX - eyesCenter[0]):] if int(tX - eyesCenter[0]) > 0 else img[:, :int(tX - eyesCenter[0])]\n",
        "\n",
        "                img = img[:int(tY - eyesCenter[1]), :] if int(tY - eyesCenter[1]) < 0 else img\n",
        "                img = img[:, :int(tX - eyesCenter[0])] if int(tX - eyesCenter[0]) < 0 else img\n",
        "\n",
        "    cTime = time.time()\n",
        "    fps = 1/(cTime - pTime)\n",
        "    pTime = cTime\n",
        "    cv2.putText(img, f'FPS: {int(fps)}', (40, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 3)\n",
        "\n",
        "    if img.shape[0] > 0 and img.shape[1] > 0:\n",
        "        cv2.imshow(\"webcam\", img)\n",
        "    cv2.imshow(\"mesh\", blank)\n",
        "    # k = cv2.waitKey(10)\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "outputs": [],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def magnify(array: np.ndarray):\n",
        "    # media = array.mean()\n",
        "    # idx_up_mean = np.where(array < media)\n",
        "    # idx_down_mean = np.where(array > media)\n",
        "    # array[idx_up_mean] = 255\n",
        "    # array[idx_down_mean] = 0\n",
        "    idx_max = np.where(array == max(array))\n",
        "    idx_not_max = np.where(array != max(array))\n",
        "    array[idx_max] = 255\n",
        "    array[idx_not_max] = 0\n",
        "    return array\n",
        "\n",
        "cam = cv2.VideoCapture('/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/MR-NIRP_Indoor/Subject1_still_940/RGB_20s/Frame%05d.pgm')\n",
        "writer = cv2.VideoWriter('/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/MR-NIRP_Indoor/Subject1_still_940/teste.avi', cv2.VideoWriter_fourcc(*'DIVX'), 30, (640, 640))\n",
        "video = np.zeros((30, 640, 640, 3))\n",
        "cam_index = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cam.read()\n",
        "\n",
        "    if ret == False:\n",
        "        print(f'O video acabou')\n",
        "        break\n",
        "\n",
        "    video[cam_index] = frame # [:,:,1]\n",
        "    cam_index += 1\n",
        "\n",
        "    if cam_index >= 30:\n",
        "        print('Magnificando ...')\n",
        "        cam_index = 0\n",
        "        array = video.copy() # np.zeros((30, 640, 640, 3))\n",
        "        for line in range(video.shape[1]):\n",
        "            for col in range(video.shape[2]):\n",
        "                if col > 150 and line > 50 and line < 550:\n",
        "                    magnified = magnify(video[:, line, col, 1]) # , channel\n",
        "                    for channel in range(video.shape[3]):\n",
        "                        array[:, line, col, channel] = magnified\n",
        "        video = np.zeros((30, 640, 640, 3))\n",
        "\n",
        "        for frame in array:\n",
        "            writer.write(frame.astype('uint8'))\n",
        "        del array\n",
        "\n",
        "cam.release()\n",
        "writer.release()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def show(img):\n",
        "    cv2.imshow('img', img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def scale_chn(img, chn, factor):\n",
        "    img = img.astype('uint16')\n",
        "    scale = int(2**(factor-1))\n",
        "    if chn == 'r':\n",
        "        b = img[:,:,0]\n",
        "        g = img[:,:,1]\n",
        "        r = np.clip(img[:,:,2]+scale-1, 0, 255)\n",
        "    if chn == 'g':\n",
        "        b = img[:,:,0]\n",
        "        g = np.clip(img[:,:,1]+scale-1, 0, 255)\n",
        "        r = img[:,:,2]\n",
        "    if chn == 'b':\n",
        "        b = np.clip(img[:,:,0]+scale-1, 0, 255)\n",
        "        g = img[:,:,1]\n",
        "        r = img[:,:,2]\n",
        "\n",
        "    return cv2.merge((b.astype('uint8'), g.astype('uint8'), r.astype('uint8'))) \n",
        "\n",
        "bayer = np.right_shift(cv2.imread('/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/MR-NIRP_Indoor/Subject1_still_940/teste/Frame00000_1.pgm',-1),8)\n",
        "\n",
        "# Perform a Bayer Reconstruction\n",
        "demosaic = cv2.cvtColor(bayer, cv2.COLOR_BAYER_BG2BGR)\n",
        "\n",
        "# demosaic = scale_chn(demosaic, 'r', 5)\n",
        "# demosaic = scale_chn(demosaic, 'r', 0)\n",
        "# demosaic = scale_chn(demosaic, 'g', 0)\n",
        "# demosaic = scale_chn(demosaic, 'g', 1)\n",
        "# demosaic = scale_chn(demosaic, 'b', 1)\n",
        "\n",
        "show(demosaic.astype('uint8'))\n",
        "demosaic"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import cv2\n",
        "from tkinter import *\n",
        "from PIL import Image, ImageTk\n",
        "\n",
        "#Creating Tkinter Window and Label\n",
        "root = Tk()\n",
        "video = Label(root)\n",
        "video.pack()\n",
        "\n",
        "#Getting video from webcam\n",
        "vid = cv2.VideoCapture(\"/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/MR-NIRP_Indoor/Subject1_still_940/RGB_20s/Frame%05d.pgm\")\n",
        "\n",
        "#Loop which display video on the label\n",
        "while(True):\n",
        "    \n",
        "    ret, frame = vid.read() #Reads the video\n",
        "\n",
        "    if ret == False:\n",
        "        print('O video terminou')\n",
        "        break\n",
        "\n",
        "    #Converting the video for Tkinter\n",
        "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
        "    img = Image.fromarray(cv2image)\n",
        "    HEIGHT, WIDTH = img.size\n",
        "    imgtk = ImageTk.PhotoImage(image=img, height=HEIGHT, width=WIDTH)\n",
        "\n",
        "    #Setting the image on the label\n",
        "    video.config(image=imgtk)\n",
        "\n",
        "    root.update() #Updates the Tkinter window\n",
        "\n",
        "vid.release()\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# vlc_instance = vlc.Instance(\"--no-xlib\")\n",
        "\n",
        "# vlc_media_instance = vlc_instance.media_player_new()\n",
        "\n",
        "# Media = vlc_instance.media_new(\n",
        "#     '/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/MR-NIRP_Indoor/Subject1_still_940/RGB_20s/Frame%05d.pgm'\n",
        "# )\n",
        "# vlc_media_instance.set_media(Media)\n",
        "# vlc_media_instance.play()\n",
        "# vlc_media_instance.__dict__"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O video terminou\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "import natsort\n",
        "import cv2\n",
        "import os\n",
        "from utils import *\n",
        "\n",
        "def demosaic_RGGB_video(video_path: str, out_path: str, video_name_pattern='Frame%05d.pgm'):\n",
        "  \"\"\"Funcao que recebe a estrutura de pasta para aplicar o demosaiced com o padrao RGGB em um video.\n",
        "\n",
        "  - O padrao (video_name_pattern) so pode ter uma string com o seguinte formatador (%d ou nenhum):\n",
        "    - (1) 'Frame%05d.pgm' no qual sera feito: pattern % 0\n",
        "\n",
        "  Keyword arguments:\n",
        "  - video_path: str -- ...\n",
        "  - out_path: str -- Caminho da pasta na qual ...\n",
        "  - video_name_pattern: str -- Padrao do nome da sequencia de imagens; ex.: 'Frame%05d.pgm' (%05d diz \\\n",
        "    que a sequencia vai ter cinco digitos, sendo que os primeiros vao ser preenchidos com zeros: Frame00000.pgm, \\\n",
        "      Frame00001.pgm, Frame00002.pgm, ...)\n",
        "\n",
        "  Return: \n",
        "    None\n",
        "  \"\"\"\n",
        "  assert os.path.isdir(out_path), \"out_path deve ser um diretorio\"\n",
        "\n",
        "  start = video_name_pattern.split('%')[0]\n",
        "  end = video_name_pattern.split('d')[-1]\n",
        "\n",
        "  def frame_filter(filename: str):\n",
        "    middle = filename.replace(start, '').replace(end, '')\n",
        "    return filename.startswith(start) and filename.endswith(end) and middle.isdigit()\n",
        "\n",
        "  if os.path.isdir(video_path) == False:\n",
        "    # get parent folder\n",
        "    video_dir = os.sep.join(video_path.split(os.sep)[:-1])\n",
        "  else:\n",
        "    video_dir = video_path\n",
        "\n",
        "  frame_names = natsort.natsorted(filter(frame_filter, os.listdir(video_dir)))\n",
        "  \n",
        "  for frame_name in frame_names:\n",
        "    frame_path = os.path.join(video_dir, frame_name)\n",
        "\n",
        "    # based on: https://gist.github.com/bbattista/8358ccafecf927ae1c58c944ab470ffb\n",
        "    frame = np.right_shift(cv2.imread(frame_path, cv2.IMREAD_UNCHANGED), 8)\n",
        "\n",
        "    # Perform a Bayer Reconstruction\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BAYER_BG2BGR).astype('uint8')\n",
        "\n",
        "    # frame = cv2.cvtColor(frame.astype('uint8'), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    cv2.imwrite(os.path.join(out_path, frame_name.replace(end, '.png')), frame)\n",
        "\n",
        "def demosaic_RGGB_dataset(dataset: Dataset, out_dataset_path: str, video_name_pattern='Frame%05d.pgm'):\n",
        "  \"\"\"Funcao que recebe a estrutura de pasta para aplicar o demosaiced em cada video com a funcao demosaic_video.\n",
        "\n",
        "  - O padrao (video_name_pattern) so pode ter uma string com o seguinte formatador (%d ou nenhum):\n",
        "    - (1) 'Frame%05d.pgm' no qual sera feito: pattern % 0\n",
        "\n",
        "  - O objeto dataset deve pertencer a uma classe que herda da classe Dataset\n",
        "  - A classe do objeto dataset deve ter implementado os metodos save_video e ppg_flatten\n",
        "\n",
        "  - Todos os caminhos vao ser verificados durante o processo, um erro vai ser emitido sem prejudicar \\\n",
        "    os outros processos caso eles nao existam.\n",
        "\n",
        "  Keyword arguments:\n",
        "  - dataset: Dataset -- Objeto que pertence a uma classe que herda da classe Dataset e implementa todos os seus metodos.\\\n",
        "    dataset.paths: Iterable -- Lista de dicionarios que contem a seguinte estrutura:\\n\n",
        "    [\n",
        "      {\n",
        "        'subject_folder_name': 'Nome do sujeito ou da pasta do sujeito', \n",
        "        'subdir_folder_name': 'Nome da pasta interna que contem o video do sujeito', \n",
        "        'video_path': 'Caminho relativo ou absoluto do video com o nome do arquivo',\n",
        "        'ppg_path': 'Caminho relativo ou absoluto do PPG em formato .mat com o nome do arquivo'\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "  - out_dataset_path: str -- Caminho da pasta na qual deseja a estrutura final do dataset output (artificial).\n",
        "  - video_name_pattern: str -- Padrao do nome da sequencia de imagens; ex.: 'Frame%05d.pgm' (%05d diz \\\n",
        "    que a sequencia vai ter cinco digitos, sendo que os primeiros vao ser preenchidos com zeros: Frame00000.pgm, \\\n",
        "      Frame00001.pgm, Frame00002.pgm, ...)\n",
        "\n",
        "  Return: \n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  assert issubclass(dataset.__class__, Dataset), \"O objeto dataset deve pertencer a uma classe que herda da classe Dataset\"\n",
        "\n",
        "  exceptions_count = 0\n",
        "\n",
        "  # para cada video que precisa ser processado\n",
        "  for path in dataset.paths:\n",
        "    subject_folder_name = path['subject_folder_name']\n",
        "    subdir_folder_name = path['subdir_folder_name']\n",
        "    video_path = path['video_path']\n",
        "\n",
        "    out_path = os.path.join(out_dataset_path, subject_folder_name, 'RGB_demosaiced')\n",
        "    make_dir(out_path)\n",
        "    \n",
        "    try:\n",
        "      assert check_video_pattern(video_path, video_name_pattern, None), f\"O seguinte video_path nao foi encontrado: {video_path}\"\n",
        "      demosaic_RGGB_video(\n",
        "        video_path=video_path,\n",
        "        out_path=out_path,\n",
        "        video_name_pattern=video_name_pattern\n",
        "      )\n",
        "    except Exception as e:\n",
        "      print_log(f\"\\nException nao mapeada: \\n{logging.traceback.format_exc()}\", None)\n",
        "      exceptions_count += 1\n",
        "\n",
        "  print_log(f\"\\nQuantidade de Exceptions: {exceptions_count}\", None)\n",
        "  print_log(f\"\\nFim da execucao: {datetime.datetime.now()}\", None)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "dataset = MRNirp()\n",
        "\n",
        "dataset.map_dataset(\n",
        "    base_dataset_path='/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste', \n",
        "    subdir_name=['RGB'], \n",
        "    video_name_pattern='Frame%05d.pgm'\n",
        ")\n",
        "\n",
        "demosaic_RGGB_dataset(\n",
        "    dataset=dataset, \n",
        "    out_dataset_path='/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste'\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantidade de Exceptions: 0\n",
            "\n",
            "Fim da execucao: 2021-07-01 16:27:56.945851\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from dataset_creator import process_dataset\n",
        "from utils import MRNirp\n",
        "\n",
        "dataset = MRNirp(image_extension='png')\n",
        "\n",
        "dataset.map_dataset(\n",
        "    base_dataset_path='/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste', \n",
        "    subdir_name=['RGB_demosaiced'], \n",
        "    video_name_pattern='Frame%05d.' + dataset.image_extension\n",
        ")\n",
        "\n",
        "process_dataset(\n",
        "    dataset=dataset,\n",
        "    out_dataset_path='/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out',\n",
        "    video_name_pattern='Frame%05d.' + dataset.image_extension\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caminho do dataset artificial: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out\n",
            "Padrao do nome dos videos: Frame%05d.png\n",
            "\n",
            "Videos que vao ser processados:\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_1/RGB_demosaiced/Frame%05d.png\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_2/RGB_demosaiced/Frame%05d.png\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_3/RGB_demosaiced/Frame%05d.png\n",
            "PPGs que vao ser processados:\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_1/PulseOX/pulseOx.mat\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_2/PulseOX/pulseOx.mat\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_3/PulseOX/pulseOx.mat\n",
            "\n",
            "========== Video sujeito_1_RGB_demosaiced ==========\n",
            "\n",
            "Processando video sujeito_1_RGB_demosaiced com os argumentos:\n",
            "video_path: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_1/RGB_demosaiced/Frame%05d.png\n",
            "out_path: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_1/RGB_demosaiced\n",
            "detector_type: FaceMesh\n",
            "save_log: True\n",
            "save_videos: True\n",
            "realtime: False\n",
            "use_threshold: True\n",
            "distortion_fixed: False\n",
            "use_frame_stabilizer: False\n",
            "use_eyes_lock: False\n",
            "use_video_stabilizer: False\n",
            "threshold_modes: {'bottom_face': 'replace', 'right_eye': 'mean', 'left_eye': 'mean'}\n",
            "dataset: <utils.MRNirp object at 0x7f400c636da0>\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_1/RGB_demosaiced was created sucessfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KProcessing... |##                              | 1/12 - 1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_1/RGB_demosaiced/bottom_face was created sucessfully\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_1/RGB_demosaiced/right_eye was created sucessfully\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_1/RGB_demosaiced/left_eye was created sucessfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KProcessing... |################################| 12/12 - 0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantidade de excecoes: 0\n",
            "\n",
            "O video acabou\n",
            "Processando PPG ...\n",
            "\n",
            "========== Video sujeito_2_RGB_demosaiced ==========\n",
            "\n",
            "Processando video sujeito_2_RGB_demosaiced com os argumentos:\n",
            "video_path: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_2/RGB_demosaiced/Frame%05d.png\n",
            "out_path: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_2/RGB_demosaiced\n",
            "detector_type: FaceMesh\n",
            "save_log: True\n",
            "save_videos: True\n",
            "realtime: False\n",
            "use_threshold: True\n",
            "distortion_fixed: False\n",
            "use_frame_stabilizer: False\n",
            "use_eyes_lock: False\n",
            "use_video_stabilizer: False\n",
            "threshold_modes: {'bottom_face': 'replace', 'right_eye': 'mean', 'left_eye': 'mean'}\n",
            "dataset: <utils.MRNirp object at 0x7f400c636da0>\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_2/RGB_demosaiced was created sucessfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KProcessing... |##                              | 1/12 - 1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_2/RGB_demosaiced/bottom_face was created sucessfully\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_2/RGB_demosaiced/right_eye was created sucessfully\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_2/RGB_demosaiced/left_eye was created sucessfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KProcessing... |################################| 12/12 - 0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantidade de excecoes: 0\n",
            "\n",
            "O video acabou\n",
            "Processando PPG ...\n",
            "\n",
            "========== Video sujeito_3_RGB_demosaiced ==========\n",
            "\n",
            "Processando video sujeito_3_RGB_demosaiced com os argumentos:\n",
            "video_path: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_teste/sujeito_3/RGB_demosaiced/Frame%05d.png\n",
            "out_path: /home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_3/RGB_demosaiced\n",
            "detector_type: FaceMesh\n",
            "save_log: True\n",
            "save_videos: True\n",
            "realtime: False\n",
            "use_threshold: True\n",
            "distortion_fixed: False\n",
            "use_frame_stabilizer: False\n",
            "use_eyes_lock: False\n",
            "use_video_stabilizer: False\n",
            "threshold_modes: {'bottom_face': 'replace', 'right_eye': 'mean', 'left_eye': 'mean'}\n",
            "dataset: <utils.MRNirp object at 0x7f400c636da0>\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_3/RGB_demosaiced was created sucessfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KProcessing... |##                              | 1/12 - 1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_3/RGB_demosaiced/bottom_face was created sucessfully\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_3/RGB_demosaiced/right_eye was created sucessfully\n",
            "/home/desafio01/Documents/Codes/bio_hmd/Dataset_MR_NIRP/dataset_out/sujeito_3/RGB_demosaiced/left_eye was created sucessfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[KProcessing... |################################| 12/12 - 0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantidade de excecoes: 0\n",
            "\n",
            "O video acabou\n",
            "Processando PPG ...\n",
            "\n",
            "Quantidade de Exceptions: 0\n",
            "\n",
            "Quantidade de VideoExceptions: 0\n",
            "\n",
            "Fim da execucao: 2021-07-02 21:01:58.258233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Eyes&MouthCropper.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "baee01580b67fcc6863d1854e328e1402443b2d5770e195dcec9bbe0aa3da30d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.10 64-bit ('hr_estimation_pipeline': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}